{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN fine tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will load the CNN , freeze the layers we want to leave untouched, and finetune the remaining ones. The image dataset was organized into an appropriate subfolder structure in the notbook called \"image_sorting.ipynb\".\n",
    "In this dataset, each image consists in three channels: one actin, one tubulin, and one DAPI, which correspond to different structures of the cells that get coloured prior to the micriscopy image capturing. Each of these channel is stored as a single image. We therefore need to reconstitute the 3D tensors from the three channels of these images before training the CNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's first import the packages we'll need\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import multiprocessing\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Inception V3 model\n",
    "model = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)\n",
    "model.eval();\n",
    "\n",
    "torch.set_num_threads(15)  # Set the number of intra-op threads\n",
    "# torch.set_num_interop_threads(15)  # Set the number of inter-op threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first try to freeze all layers except for the last fully connected layer, and finetune this one only. We will then try an alternative approach, by visualizing the features detected by the CNN, freeze layers that detect high level features, and fine tune those which detect rather low level ones. The features visualization is performed in notebook \"CNN_features_visualization\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#replace the last fc layer\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 13) \n",
    "\n",
    "#set parameters of last fc open for fine tunning\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define a class that aims at reconsituting the 3 channels of the picture (Actin, Tubulin and DAPI). In the rest of the code, we will name these channels ATD, including in the variable names, to not cause potential confusion with RGB channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATDImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        \n",
    "        self.image_paths = []\n",
    "        for cls in self.classes:\n",
    "            a_path = os.path.join(root_dir, cls, 'actin')\n",
    "            t_path = os.path.join(root_dir, cls, 'tubulin')\n",
    "            d_path = os.path.join(root_dir, cls, 'dapi')\n",
    "            for img_name in os.listdir(a_path):\n",
    "                self.image_paths.append((os.path.join(a_path, img_name),\n",
    "                                         os.path.join(t_path, img_name),\n",
    "                                         os.path.join(d_path, img_name),\n",
    "                                         cls))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        a_path, t_path, d_path, cls = self.image_paths[idx]\n",
    "        a_img = Image.open(a_path)\n",
    "        t_img = Image.open(t_path)\n",
    "        d_img = Image.open(d_path)\n",
    "\n",
    "        # Normalize pixel values from 0-65535 to 0-255\n",
    "        a_img = a_img.point(lambda p: p * (255.0 / 65535.0))\n",
    "        t_img = t_img.point(lambda p: p * (255.0 / 65535.0))\n",
    "        d_img = d_img.point(lambda p: p * (255.0 / 65535.0))\n",
    "\n",
    "        # Convert images to grayscale\n",
    "        a_img = a_img.convert('L')\n",
    "        t_img = t_img.convert('L')\n",
    "        d_img = d_img.convert('L')\n",
    "        \n",
    "        img = Image.merge('RGB', (a_img, t_img, d_img))\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img, self.classes.index(cls)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "dataset = ATDImageDataset(root_dir='images/sorted_reduced/train', transform = transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 9.687756374887613e-09\n",
      "Std: 1.1472706340498462e-08\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to store the sum and std of pixel values\n",
    "mean = 0.0\n",
    "std = 0.0\n",
    "num_pixels = 0\n",
    "\n",
    "# Mean: 9.687756374887613e-09\n",
    "# Std: 1.1472706340498462e-08\n",
    "\n",
    "# Iterate through the dataset\n",
    "for images, _ in dataloader:\n",
    "    batch_size, num_channels, height, width = images.shape\n",
    "    num_pixels += batch_size * height * width\n",
    "    mean += images.mean(axis=(0, 2, 3)).sum()\n",
    "    std += images.std(axis=(0, 2, 3)).sum()\n",
    "\n",
    "# Calculate the mean and std\n",
    "mean /= num_pixels\n",
    "std /= num_pixels\n",
    "\n",
    "print(f'Mean: {mean}')\n",
    "print(f'Std: {std}')\n",
    "\n",
    "# Updated transform with normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# Reload dataset with normalization\n",
    "dataset = ATDImageDataset(root_dir='images/sorted_reduced/train', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model with lr = 0.001, momentum = 0.9\n",
      "done for img 0, epoch 1\n",
      "done for img 10, epoch 1\n",
      "done for img 20, epoch 1\n",
      "done for img 30, epoch 1\n",
      "Epoch [1/10], Loss: 2.5516644653521086\n"
     ]
    }
   ],
   "source": [
    "def train_model(lr, momentum):\n",
    "    print('training model with lr = {}, momentum = {}'.format(lr, momentum))\n",
    "    # Define  loss function and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    #define the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    #list to store losses\n",
    "    losses = []\n",
    "\n",
    "    #define the number of epochs\n",
    "    num_epochs = 10\n",
    "\n",
    "    # Training loop\n",
    "\n",
    "    for epoch in range(num_epochs):  \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(dataloader):  \n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the appropriate device\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i%10 == 0:\n",
    "                print('done for img {}, epoch {}'.format(i, epoch+1))\n",
    "        losses.append(running_loss / len(dataloader))\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader)}')\n",
    "\n",
    "    return model, losses\n",
    "\n",
    "for lr in [0.001, 0.01, 0.1, 1]:\n",
    "    model_name = 1\n",
    "    model, losses = train_model(lr, 0.9)\n",
    "    try:\n",
    "        torch.save(model.state_dict(), 'models/model{}/cnn'.format(model_name))\n",
    "    except:\n",
    "        os.makedirs(os.path.dirname('models/model{}/cnn'.format(model_name)))\n",
    "        torch.save(model.state_dict(), 'models/model{}/cnn'.format(model_name))\n",
    "    with open('models/model{}/losses'.format(model_name), \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(losses, fp)\n",
    "    model_name += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.3362345695495605,\n",
       " 2.2832021713256836,\n",
       " 2.6072685718536377,\n",
       " 2.216726541519165,\n",
       " 2.365002155303955,\n",
       " 2.576688528060913,\n",
       " 2.507329225540161,\n",
       " 2.329982042312622,\n",
       " 2.2601375579833984,\n",
       " 2.437709331512451,\n",
       " 2.433056592941284,\n",
       " 2.370053768157959,\n",
       " 2.40954852104187,\n",
       " 2.4517765045166016,\n",
       " 2.2286007404327393,\n",
       " 2.378108501434326,\n",
       " 2.5210256576538086,\n",
       " 2.4442620277404785,\n",
       " 2.568758010864258,\n",
       " 2.4824845790863037,\n",
       " 2.4054033756256104,\n",
       " 2.383650064468384,\n",
       " 2.459444522857666,\n",
       " 2.612511157989502,\n",
       " 2.5745246410369873,\n",
       " 2.4844417572021484,\n",
       " 2.4969847202301025,\n",
       " 2.463207244873047,\n",
       " 2.477919816970825,\n",
       " 2.461578607559204,\n",
       " 2.5261142253875732,\n",
       " 2.5680058002471924,\n",
       " 2.560798406600952,\n",
       " 2.3730664253234863,\n",
       " 2.3560421466827393,\n",
       " 2.4384782314300537,\n",
       " 2.4363255500793457,\n",
       " 2.6722397804260254,\n",
       " 2.5517780780792236,\n",
       " 2.4916865825653076,\n",
       " 2.394786834716797,\n",
       " 2.641256332397461,\n",
       " 2.452791690826416,\n",
       " 2.399306535720825,\n",
       " 2.3660695552825928,\n",
       " 2.3384218215942383,\n",
       " 2.221323251724243,\n",
       " 2.5113577842712402,\n",
       " 2.295729398727417,\n",
       " 2.584869146347046,\n",
       " 2.542513132095337,\n",
       " 2.597883462905884,\n",
       " 2.487623929977417,\n",
       " 2.491811752319336,\n",
       " 2.4479072093963623,\n",
       " 2.3871026039123535,\n",
       " 2.278157949447632,\n",
       " 2.446434736251831,\n",
       " 2.610778331756592,\n",
       " 2.4240193367004395,\n",
       " 2.2938716411590576,\n",
       " 2.3801820278167725,\n",
       " 2.3159050941467285,\n",
       " 2.4081428050994873,\n",
       " 2.467439889907837,\n",
       " 2.586059331893921,\n",
       " 2.5905213356018066,\n",
       " 2.5375726222991943,\n",
       " 2.3821299076080322,\n",
       " 2.305091381072998,\n",
       " 2.5314383506774902,\n",
       " 2.5140461921691895,\n",
       " 2.429506540298462,\n",
       " 2.3885457515716553,\n",
       " 2.4975087642669678,\n",
       " 2.3367745876312256,\n",
       " 2.242938995361328,\n",
       " 2.635023355484009,\n",
       " 2.427328109741211,\n",
       " 2.4844536781311035,\n",
       " 2.481198787689209,\n",
       " 2.515333652496338,\n",
       " 2.444077491760254,\n",
       " 2.2528865337371826,\n",
       " 2.395026683807373,\n",
       " 2.534764528274536,\n",
       " 2.3300693035125732,\n",
       " 2.455890655517578,\n",
       " 2.364696741104126,\n",
       " 2.3914220333099365,\n",
       " 2.5354857444763184,\n",
       " 2.3338699340820312,\n",
       " 2.4285049438476562,\n",
       " 2.2698287963867188,\n",
       " 2.405001163482666,\n",
       " 2.2334232330322266,\n",
       " 2.196733236312866,\n",
       " 2.407428503036499,\n",
       " 2.330719470977783,\n",
       " 2.344982385635376,\n",
       " 2.353057622909546,\n",
       " 2.3386337757110596,\n",
       " 2.4357824325561523,\n",
       " 2.283574342727661,\n",
       " 2.352334499359131,\n",
       " 2.380817413330078,\n",
       " 2.367542028427124,\n",
       " 2.4713284969329834,\n",
       " 2.308704376220703,\n",
       " 2.1992108821868896,\n",
       " 2.209934949874878,\n",
       " 2.410703182220459,\n",
       " 2.3702149391174316,\n",
       " 2.392709970474243,\n",
       " 2.5862600803375244,\n",
       " 2.462620496749878,\n",
       " 2.7088918685913086,\n",
       " 2.503541946411133,\n",
       " 2.3758468627929688,\n",
       " 2.176870107650757,\n",
       " 2.296888589859009,\n",
       " 2.3566958904266357,\n",
       " 2.323023796081543,\n",
       " 2.5308971405029297,\n",
       " 2.4450855255126953,\n",
       " 2.4024970531463623,\n",
       " 2.3779711723327637,\n",
       " 2.3257217407226562,\n",
       " 2.4648590087890625,\n",
       " 2.5119664669036865,\n",
       " 2.3332581520080566,\n",
       " 2.244926691055298,\n",
       " 2.17913556098938,\n",
       " 2.3010456562042236,\n",
       " 2.3077080249786377,\n",
       " 2.4396114349365234,\n",
       " 2.2942521572113037,\n",
       " 2.631972074508667,\n",
       " 2.541067123413086,\n",
       " 2.3840603828430176,\n",
       " 2.4304614067077637,\n",
       " 2.4581329822540283,\n",
       " 2.2848904132843018,\n",
       " 2.467712163925171,\n",
       " 2.475562572479248,\n",
       " 2.5588979721069336,\n",
       " 2.4788711071014404,\n",
       " 2.4226298332214355,\n",
       " 2.3538944721221924,\n",
       " 2.711146831512451,\n",
       " 2.4952213764190674,\n",
       " 2.5274698734283447,\n",
       " 2.3873531818389893,\n",
       " 2.3572585582733154,\n",
       " 2.493562936782837,\n",
       " 2.30159068107605,\n",
       " 2.3532869815826416,\n",
       " 2.368774175643921,\n",
       " 2.5258123874664307,\n",
       " 2.3486928939819336,\n",
       " 2.4809653759002686,\n",
       " 2.334939956665039,\n",
       " 2.539158821105957,\n",
       " 2.26503324508667,\n",
       " 2.4073519706726074,\n",
       " 2.601238965988159,\n",
       " 2.2428009510040283,\n",
       " 2.2588415145874023,\n",
       " 2.394394874572754,\n",
       " 2.5945935249328613,\n",
       " 2.541870355606079,\n",
       " 2.264819860458374,\n",
       " 2.5712168216705322,\n",
       " 2.4131760597229004,\n",
       " 2.45767879486084,\n",
       " 2.5257980823516846,\n",
       " 2.467329263687134,\n",
       " 2.310323476791382,\n",
       " 2.255653142929077,\n",
       " 2.201470136642456,\n",
       " 2.3781442642211914,\n",
       " 2.6281960010528564,\n",
       " 2.341919422149658,\n",
       " 2.638434410095215,\n",
       " 2.3634445667266846,\n",
       " 2.5393733978271484,\n",
       " 2.5177090167999268,\n",
       " 2.533350944519043,\n",
       " 2.4066526889801025,\n",
       " 2.4279727935791016,\n",
       " 2.2509353160858154,\n",
       " 2.4121806621551514,\n",
       " 2.220059871673584,\n",
       " 2.4459481239318848,\n",
       " 2.403205156326294,\n",
       " 2.3858110904693604,\n",
       " 2.4397199153900146,\n",
       " 2.2865121364593506,\n",
       " 2.530247449874878,\n",
       " 2.3608896732330322,\n",
       " 2.3061461448669434,\n",
       " 2.4802064895629883,\n",
       " 2.33327317237854,\n",
       " 2.4313607215881348,\n",
       " 2.44081449508667,\n",
       " 2.6707072257995605,\n",
       " 2.3577702045440674,\n",
       " 2.522334575653076,\n",
       " 2.3599324226379395,\n",
       " 2.509150266647339,\n",
       " 2.3390865325927734,\n",
       " 2.1764638423919678,\n",
       " 2.6594653129577637,\n",
       " 2.6060967445373535,\n",
       " 2.3554930686950684,\n",
       " 2.5496180057525635,\n",
       " 2.4040377140045166,\n",
       " 2.408360719680786,\n",
       " 2.314631462097168,\n",
       " 2.381183624267578,\n",
       " 2.0860142707824707,\n",
       " 2.459599018096924,\n",
       " 2.4338037967681885,\n",
       " 2.4250364303588867,\n",
       " 2.408963203430176,\n",
       " 2.5740623474121094,\n",
       " 2.1793763637542725,\n",
       " 2.403841018676758,\n",
       " 2.4558165073394775,\n",
       " 2.242647647857666,\n",
       " 2.3478736877441406,\n",
       " 2.389050006866455,\n",
       " 2.2625441551208496,\n",
       " 2.319395065307617,\n",
       " 2.450052261352539,\n",
       " 2.525733470916748,\n",
       " 2.4899845123291016,\n",
       " 2.4267048835754395,\n",
       " 2.256735324859619,\n",
       " 1.9987703561782837,\n",
       " 2.466777801513672,\n",
       " 2.3051204681396484,\n",
       " 2.429945945739746,\n",
       " 2.4145548343658447,\n",
       " 2.331498384475708,\n",
       " 2.62644624710083,\n",
       " 2.3869991302490234,\n",
       " 2.443788766860962,\n",
       " 2.312699317932129,\n",
       " 2.318941593170166,\n",
       " 2.5561437606811523,\n",
       " 2.5367822647094727,\n",
       " 2.160154342651367,\n",
       " 2.5949790477752686,\n",
       " 2.2808759212493896,\n",
       " 2.486541748046875,\n",
       " 2.4411182403564453,\n",
       " 2.5497281551361084,\n",
       " 2.4204001426696777,\n",
       " 2.55896258354187,\n",
       " 2.6894915103912354,\n",
       " 2.28645658493042,\n",
       " 2.5481116771698,\n",
       " 2.506273031234741,\n",
       " 2.3361003398895264,\n",
       " 2.4180045127868652,\n",
       " 2.3732287883758545,\n",
       " 2.491959810256958,\n",
       " 2.637542247772217,\n",
       " 2.26395320892334,\n",
       " 2.4216959476470947,\n",
       " 2.353818416595459,\n",
       " 2.4986820220947266,\n",
       " 2.3772497177124023,\n",
       " 2.203223705291748,\n",
       " 2.3342971801757812,\n",
       " 2.371347188949585,\n",
       " 2.42950701713562,\n",
       " 2.437241554260254,\n",
       " 2.5143706798553467,\n",
       " 2.1402151584625244,\n",
       " 2.4481379985809326,\n",
       " 2.585294723510742,\n",
       " 2.449586868286133,\n",
       " 2.5377743244171143,\n",
       " 2.4253792762756348,\n",
       " 2.3249549865722656,\n",
       " 2.2764101028442383,\n",
       " 2.114875078201294,\n",
       " 2.393681764602661,\n",
       " 2.4095823764801025,\n",
       " 2.408658504486084,\n",
       " 2.4944655895233154,\n",
       " 2.4991390705108643,\n",
       " 2.393359899520874,\n",
       " 2.2811412811279297,\n",
       " 2.3580594062805176,\n",
       " 2.187746286392212,\n",
       " 2.5009360313415527,\n",
       " 2.1288247108459473,\n",
       " 2.414919853210449,\n",
       " 2.3948886394500732,\n",
       " 2.5044608116149902,\n",
       " 2.452695608139038,\n",
       " 2.36807918548584,\n",
       " 2.3235723972320557,\n",
       " 2.2339653968811035,\n",
       " 2.4667415618896484,\n",
       " 2.2478978633880615,\n",
       " 2.320035219192505,\n",
       " 2.4849321842193604,\n",
       " 2.3164258003234863,\n",
       " 2.6982967853546143,\n",
       " 2.4771413803100586,\n",
       " 2.4667935371398926,\n",
       " 2.3126113414764404,\n",
       " 2.5512285232543945,\n",
       " 2.4983763694763184,\n",
       " 2.3171889781951904,\n",
       " 2.290628433227539,\n",
       " 2.433009624481201,\n",
       " 2.4400227069854736,\n",
       " 2.3760969638824463,\n",
       " 2.439681053161621,\n",
       " 2.2690138816833496,\n",
       " 2.4392905235290527,\n",
       " 2.4452755451202393,\n",
       " 2.259989023208618,\n",
       " 2.3167924880981445,\n",
       " 2.492781162261963,\n",
       " 2.4350202083587646,\n",
       " 2.4572594165802,\n",
       " 2.428231954574585,\n",
       " 2.7460274696350098,\n",
       " 2.4950268268585205,\n",
       " 2.4521660804748535,\n",
       " 2.537844181060791,\n",
       " 2.4979052543640137,\n",
       " 2.417388916015625,\n",
       " 2.4692788124084473,\n",
       " 2.1841647624969482,\n",
       " 2.2550673484802246,\n",
       " 2.4983465671539307,\n",
       " 2.4615893363952637,\n",
       " 2.2914061546325684,\n",
       " 2.297908306121826,\n",
       " 2.3553388118743896,\n",
       " 2.400691509246826,\n",
       " 2.5412375926971436,\n",
       " 2.409346580505371,\n",
       " 2.453895330429077,\n",
       " 2.2633719444274902,\n",
       " 2.459188938140869,\n",
       " 2.3016531467437744,\n",
       " 2.4176433086395264,\n",
       " 2.3548483848571777,\n",
       " 2.343397855758667,\n",
       " 2.339872121810913,\n",
       " 2.2733492851257324,\n",
       " 2.0157318115234375,\n",
       " 2.3789069652557373,\n",
       " 2.261418104171753,\n",
       " 2.417630910873413,\n",
       " 2.550074577331543,\n",
       " 2.387044906616211,\n",
       " 2.3057003021240234,\n",
       " 2.386983871459961,\n",
       " 2.5701794624328613,\n",
       " 2.2779135704040527,\n",
       " 2.5117576122283936,\n",
       " 2.369849681854248,\n",
       " 2.483262777328491,\n",
       " 2.564256191253662,\n",
       " 2.2680602073669434,\n",
       " 2.3181285858154297,\n",
       " 2.3424875736236572,\n",
       " 2.2050604820251465,\n",
       " 2.348350763320923,\n",
       " 2.390932559967041,\n",
       " 2.5623741149902344,\n",
       " 2.404191732406616,\n",
       " 2.4192724227905273,\n",
       " 2.379119396209717,\n",
       " 2.583127975463867,\n",
       " 2.4301838874816895,\n",
       " 2.423125982284546,\n",
       " 2.2587058544158936,\n",
       " 2.331624984741211,\n",
       " 2.364572286605835,\n",
       " 2.5378634929656982,\n",
       " 2.4964327812194824,\n",
       " 2.478161334991455,\n",
       " 2.02718448638916,\n",
       " 2.521824836730957,\n",
       " 2.3765313625335693,\n",
       " 2.457090377807617,\n",
       " 2.4983327388763428,\n",
       " 2.417126417160034,\n",
       " 2.34029221534729,\n",
       " 2.2213072776794434,\n",
       " 2.4547576904296875,\n",
       " 2.018066883087158,\n",
       " 2.4661412239074707,\n",
       " 2.2823116779327393,\n",
       " 2.550321578979492,\n",
       " 2.2850148677825928,\n",
       " 2.3986763954162598,\n",
       " 2.583944797515869,\n",
       " 2.2466938495635986,\n",
       " 2.486410617828369,\n",
       " 2.268906831741333,\n",
       " 2.5600132942199707,\n",
       " 2.3738820552825928,\n",
       " 2.2854385375976562,\n",
       " 2.64182186126709,\n",
       " 2.4640374183654785,\n",
       " 2.4528005123138428,\n",
       " 2.443052291870117,\n",
       " 2.484469175338745,\n",
       " 2.231233835220337,\n",
       " 2.3361029624938965,\n",
       " 2.4427943229675293,\n",
       " 2.5842931270599365,\n",
       " 2.592888593673706,\n",
       " 2.251584768295288,\n",
       " 2.456782102584839,\n",
       " 2.613919734954834,\n",
       " 2.499743938446045,\n",
       " 2.4237208366394043,\n",
       " 2.459404945373535,\n",
       " 2.5285515785217285,\n",
       " 2.4568400382995605,\n",
       " 2.4964375495910645,\n",
       " 2.279052495956421,\n",
       " 2.3525681495666504,\n",
       " 2.3288512229919434,\n",
       " 2.248931884765625,\n",
       " 2.422379732131958,\n",
       " 2.3211774826049805,\n",
       " 2.641580820083618,\n",
       " 2.2687318325042725,\n",
       " 2.2392876148223877,\n",
       " 2.5946664810180664,\n",
       " 2.2316904067993164,\n",
       " 2.667374849319458,\n",
       " 2.273254871368408,\n",
       " 2.2815380096435547,\n",
       " 2.4767696857452393,\n",
       " 2.4915575981140137,\n",
       " 2.522725820541382,\n",
       " 2.368821859359741,\n",
       " 2.150076389312744,\n",
       " 2.4281411170959473,\n",
       " 2.4321460723876953,\n",
       " 2.602768659591675,\n",
       " 2.776866912841797,\n",
       " 2.6265041828155518,\n",
       " 2.5130763053894043,\n",
       " 2.4692280292510986,\n",
       " 2.5021204948425293,\n",
       " 2.3469648361206055,\n",
       " 2.453394889831543,\n",
       " 2.372499942779541,\n",
       " 2.4724931716918945,\n",
       " 2.3448030948638916,\n",
       " 2.1842567920684814,\n",
       " 2.419544219970703,\n",
       " 2.703991651535034,\n",
       " 2.7100393772125244,\n",
       " 2.3273322582244873,\n",
       " 2.3839073181152344,\n",
       " 2.564113140106201,\n",
       " 2.3860058784484863,\n",
       " 2.2848477363586426,\n",
       " 2.4775290489196777,\n",
       " 2.385955810546875,\n",
       " 2.3431410789489746,\n",
       " 2.4235737323760986,\n",
       " 2.4172303676605225,\n",
       " 2.1433730125427246,\n",
       " 2.4220895767211914,\n",
       " 2.2332260608673096,\n",
       " 2.333338975906372,\n",
       " 2.3380656242370605,\n",
       " 2.2530856132507324,\n",
       " 2.3282887935638428,\n",
       " 2.6527743339538574,\n",
       " 2.372091770172119,\n",
       " 2.254824161529541,\n",
       " 2.5642271041870117,\n",
       " 2.3301732540130615,\n",
       " 2.5023281574249268,\n",
       " 2.471062421798706,\n",
       " 2.5536081790924072,\n",
       " 2.4068634510040283,\n",
       " 2.3866896629333496,\n",
       " 2.3129355907440186,\n",
       " 2.4161698818206787,\n",
       " 2.3596673011779785,\n",
       " 2.440514087677002,\n",
       " 2.299792766571045,\n",
       " 2.631763219833374,\n",
       " 2.4606776237487793,\n",
       " 2.318148136138916,\n",
       " 2.3802683353424072,\n",
       " 2.657681941986084,\n",
       " 2.3773550987243652,\n",
       " 2.4612579345703125,\n",
       " 2.305128335952759,\n",
       " 2.246105909347534,\n",
       " 2.3755228519439697,\n",
       " 2.336519479751587,\n",
       " 2.33583664894104,\n",
       " 2.6228349208831787,\n",
       " 2.5169084072113037,\n",
       " 2.6386799812316895,\n",
       " 2.4642162322998047,\n",
       " 2.2494630813598633,\n",
       " 2.3338868618011475,\n",
       " 2.395672082901001,\n",
       " 2.35795259475708,\n",
       " 2.3558847904205322,\n",
       " 2.5362210273742676,\n",
       " 2.2745609283447266,\n",
       " 2.4208905696868896,\n",
       " 2.2592859268188477,\n",
       " 2.4609246253967285,\n",
       " 2.419116258621216,\n",
       " 2.5602681636810303,\n",
       " 2.240910530090332,\n",
       " 2.6233127117156982,\n",
       " 2.52085280418396,\n",
       " 2.3126819133758545,\n",
       " 2.427523136138916,\n",
       " 2.3752760887145996,\n",
       " 2.257281541824341,\n",
       " 2.5019431114196777,\n",
       " 2.265951156616211,\n",
       " 2.5812551975250244,\n",
       " 2.5333352088928223,\n",
       " 2.413099765777588,\n",
       " 2.4334919452667236,\n",
       " 2.3546512126922607,\n",
       " 2.524914503097534,\n",
       " 2.7343225479125977,\n",
       " 2.3476598262786865,\n",
       " 2.5429153442382812,\n",
       " 2.5945868492126465,\n",
       " 2.1142985820770264,\n",
       " 2.2599472999572754,\n",
       " 2.210527181625366,\n",
       " 2.327169418334961,\n",
       " 2.4677207469940186,\n",
       " 2.3271548748016357,\n",
       " 2.2477123737335205,\n",
       " 2.443544626235962,\n",
       " 2.4503002166748047,\n",
       " 2.171496629714966,\n",
       " 2.572047472000122,\n",
       " 2.3256618976593018,\n",
       " 2.3850040435791016,\n",
       " 2.217252731323242,\n",
       " 2.619777202606201,\n",
       " 2.7757790088653564,\n",
       " 2.4078896045684814,\n",
       " 2.1499221324920654,\n",
       " 2.0023446083068848,\n",
       " 2.2738709449768066,\n",
       " 2.4434831142425537,\n",
       " 2.3179898262023926,\n",
       " 2.2951157093048096,\n",
       " 2.346453905105591,\n",
       " 2.3325839042663574,\n",
       " 2.251469373703003,\n",
       " 2.386740207672119,\n",
       " 2.6132586002349854,\n",
       " 2.2675373554229736,\n",
       " 2.5075879096984863,\n",
       " 2.5965449810028076,\n",
       " 2.394688606262207,\n",
       " 2.29990816116333,\n",
       " 2.4837701320648193,\n",
       " 2.4053807258605957,\n",
       " 2.376847505569458,\n",
       " 2.332453727722168,\n",
       " 2.6985840797424316,\n",
       " 2.436105728149414,\n",
       " 2.537605047225952,\n",
       " 2.58327579498291,\n",
       " 2.42608904838562,\n",
       " 2.3520703315734863,\n",
       " 2.614751100540161,\n",
       " 2.3242413997650146,\n",
       " 2.5709474086761475,\n",
       " 2.528049945831299,\n",
       " 2.1938695907592773,\n",
       " 2.4494776725769043,\n",
       " 2.427090883255005,\n",
       " 2.430558681488037,\n",
       " 2.2097010612487793]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
